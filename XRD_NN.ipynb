{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The NN-Model was trained on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import keras as K\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import find_peaks_cwt\n",
    "from scipy import interpolate\n",
    "\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.load('Xall.npy')\n",
    "Y=np.load('Yall.npy')\n",
    "print(X.shape)\n",
    "\n",
    "nc = 29  # 36 classes\n",
    "ny = 180 # row nr of xrd-pattern\n",
    "y_all = np.zeros((nc,ny))\n",
    "xnew = np.arange(0,90,0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## During testing several NN-architectures, simple dense networks performed better than 1D convolutional nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "\n",
    "#init=K.initializers.GlorotUniform(seed=12)\n",
    "init=K.initializers.HeUniform(seed=123)\n",
    "\n",
    "wreg1 = K.regularizers.l2(l=2e-3) \n",
    "wreg2 = K.regularizers.l2(l=0)\n",
    "\n",
    "model.add(K.layers.Dense(512,activation='relu',\n",
    "          kernel_initializer=init,name='d1', input_shape=(180,)))\n",
    "model.add(K.layers.Dropout(0.1))\n",
    "\n",
    "model.add(K.layers.Dense(256,activation='relu',name='d2'))\n",
    "model.add(K.layers.Dropout(0.1))\n",
    "\n",
    "model.add(K.layers.Dense(256,activation='relu',kernel_regularizer=wreg2,name='d3'))\n",
    "model.add(K.layers.Dropout(0.1))\n",
    "\n",
    "model.add(K.layers.Dense(128,kernel_regularizer=wreg2,name='d4'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(K.layers.Dense(96,kernel_regularizer=wreg2,name='d5'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(K.layers.Dense(96,kernel_regularizer=wreg2,name='d6'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(K.layers.Dense(48,kernel_regularizer=wreg2,name='d7'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(K.layers.Dense(nc, activation='sigmoid',name='d8',kernel_regularizer=wreg1))\n",
    "\n",
    "optimizer = K.optimizers.Adam(lr=0.6e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy','binary_crossentropy'], \n",
    "              )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit #monitor progess with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = K.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "chkpt_callback=tf.keras.callbacks.ModelCheckpoint('./model.hdf5', monitor='val_accuracy', \n",
    "                                 verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(X,Y,batch_size=BATCH_SIZE, \n",
    "          epochs=100, verbose=1,validation_split=0.1, \n",
    "          shuffle=True, \n",
    "          callbacks=[tensorboard_callback,chkpt_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exptected validation accuracy approx. 60% (it's better than it sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly testing some predictions on whole (train+val) sample set\n",
    "### (It would be better to create some 'fresh' test-data for this, as hyperparameters might be biased toward the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo=np.random.randint(1,len(Y))\n",
    "print(foo)\n",
    "Xtest=X[foo]\n",
    "plt.plot(Xtest)\n",
    "print('ACTUAL:')\n",
    "print(Y[foo])\n",
    "print(np.argwhere(Y[foo]==1))\n",
    "\n",
    "foo = np.expand_dims(Xtest, axis=-1).T\n",
    "print('PREDICTION:')\n",
    "bar=((model.predict(foo) > 0.5).astype(\"int32\")).squeeze()\n",
    "print(bar)\n",
    "print(np.argwhere(bar==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo: \n",
    "- try RNNs\n",
    "- use generator during fitting instead of precomputed data to increase training set (especially ternary systems are under-represented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
